{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a, b, seed):\n",
    "    'Shuffle two array in the same random order'\n",
    "    rand_state = np.random.RandomState(seed)\n",
    "    rand_state.shuffle(a)\n",
    "    rand_state.seed(seed)\n",
    "    rand_state.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sample_ID_list, labels, batch_size):\n",
    "    'Yield training data if model.fit_generator request it'\n",
    "    # Init output arrays\n",
    "    batch_features = np.zeros((batch_size, 15, 15, 13))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    \n",
    "    # Randomly shuffle samples (training order changes in each epoch)\n",
    "    rand_state = np.random.RandomState(13991)\n",
    "    rand_state.shuffle(sample_ID_list)\n",
    "    \n",
    "    # Prepare training data packets\n",
    "    index = 0\n",
    "    while True:\n",
    "        # Generate IDs of the batch\n",
    "        start_index = index*batch_size\n",
    "        end_index = (index+1)*batch_size\n",
    "        list_IDs_temp = sample_ID_list[start_index:end_index]\n",
    "        \n",
    "        # Load data from the selected sample files\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            batch_features[i, ] = np.load('data/ID' + str(ID) + '.npy')\n",
    "            batch_labels[i] = labels[ID]\n",
    "        \n",
    "        yield batch_features, batch_labels\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "dummy_data = np.zeros((46, 501, 497, 13))\n",
    "sample_segment_dim = (15,15)\n",
    "\n",
    "# Calculate number of samples\n",
    "sample_number_per_column = dummy_data.shape[1] - sample_segment_dim[0] + 1\n",
    "sample_number_per_row = dummy_data.shape[2] - sample_segment_dim[1] + 1\n",
    "#sample_number = dummy_data.shape[0] * sample_number_per_column * sample_number_per_row\n",
    "sample_number = 100\n",
    "print(sample_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = np.arange(sample_number)\n",
    "labels = np.zeros(sample_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample patches from the big images\n",
    "# Change this code to snip samples from the images!\n",
    "dummy_sample = np.zeros((15,15,13))\n",
    "for x in range(0, sample_number):\n",
    "    np.save('data/ID' + str(ID_list[x]) + '.npy', dummy_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Validation sets\n",
    "shuffle(ID_list, labels, 12045)\n",
    "\n",
    "validation_split = 0.2\n",
    "split_index = int((1-validation_split) * sample_number)\n",
    "\n",
    "train_ID_list = ID_list[:split_index]\n",
    "validation_ID_list = ID_list[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 13, 13, 4)\n",
      "(None, 676)\n",
      "(None, 16)\n"
     ]
    }
   ],
   "source": [
    "# Create NN model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(4, (3,3), strides=(1, 1), input_shape=(15,15,13)))\n",
    "print(model.output_shape)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "print(model.output_shape)\n",
    "model.add(tf.keras.layers.Dense(3, activation=\"softmax\"))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 1.0842 - val_loss: 1.0482\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.0093 - val_loss: 0.9295\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8645 - val_loss: 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16657227898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "train_batch_size = 21\n",
    "validation_batch_size = 11\n",
    "\n",
    "model.fit_generator(generator(train_ID_list, labels, train_batch_size),\n",
    "                    steps_per_epoch=math.ceil(split_index/train_batch_size),\n",
    "                    epochs=3,\n",
    "                    validation_data=generator(validation_ID_list, labels, validation_batch_size),\n",
    "                    validation_steps=math.ceil(validation_split*sample_number/validation_batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning tensorflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[-1 -2 -3 -4 -5]\n"
     ]
    }
   ],
   "source": [
    "# Create dummy data\n",
    "array_a = np.array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n",
    "array_b = np.array([-1, -2, -3, -4, -5])\n",
    "print(array_a)\n",
    "print(array_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]  ,  -1\n",
      "[2 3]  ,  -2\n",
      "[4 5]  ,  -3\n",
      "[6 7]  ,  -4\n",
      "[8 9]  ,  -5\n"
     ]
    }
   ],
   "source": [
    "# Define tensorflow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((array_a, array_b))\n",
    "for a,b in dataset:\n",
    "    print(a.numpy(), ' , ', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.6931472]  ,  -1\n",
      "[1.0986123 1.3862944]  ,  -2\n",
      "[1.609438  1.7917595]  ,  -3\n",
      "[1.9459101 2.0794415]  ,  -4\n",
      "[2.1972246 2.3025851]  ,  -5\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Apply Python functions\n",
    "def tf_some_python_function(input_a, input_b):\n",
    "    [input_a,] = tf.py_function(np.log1p, [input_a], [tf.float32])\n",
    "    return input_a, input_b\n",
    "\n",
    "mapped_dataset = dataset.map(tf_some_python_function)\n",
    "\n",
    "for a,b in mapped_dataset:\n",
    "    print(a.numpy(), ' , ', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batched data, keep partial:\n",
      "batch 0\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "[-1 -2]\n",
      "batch 1\n",
      "[[4 5]\n",
      " [6 7]]\n",
      "[-3 -4]\n",
      "batch 2\n",
      "[[8 9]]\n",
      "[-5]\n",
      "\n",
      "Batched data, keep partial, repeat:\n",
      "batch 0\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "[-1 -2]\n",
      "batch 1\n",
      "[[4 5]\n",
      " [6 7]]\n",
      "[-3 -4]\n",
      "batch 2\n",
      "[[8 9]]\n",
      "[-5]\n",
      "batch 3\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "[-1 -2]\n",
      "batch 4\n",
      "[[4 5]\n",
      " [6 7]]\n",
      "[-3 -4]\n"
     ]
    }
   ],
   "source": [
    "# Batches\n",
    "batch_size = 2\n",
    "take_n_batch = 5\n",
    "\n",
    "print('\\nBatched data, keep partial:')            \n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "\n",
    "for i, batch in enumerate(batched_dataset.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())\n",
    "        \n",
    "print('\\nBatched data, keep partial, repeat:')            \n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "batched_dataset = batched_dataset.repeat()\n",
    "\n",
    "for i, batch in enumerate(batched_dataset.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "[[8 9]\n",
      " [6 7]]\n",
      "[-5 -4]\n",
      "batch 1\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "[-1 -2]\n",
      "batch 2\n",
      "[[4 5]]\n",
      "[-3]\n",
      "batch 3\n",
      "[[4 5]\n",
      " [0 1]]\n",
      "[-3 -1]\n",
      "batch 4\n",
      "[[6 7]\n",
      " [8 9]]\n",
      "[-4 -5]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "# shuffle, batch, repeat order should be kept\n",
    "shuffled = dataset.shuffle(buffer_size=100)\n",
    "shuffled = shuffled.batch(batch_size)\n",
    "shuffled = shuffled.repeat()\n",
    "for i, batch in enumerate(shuffled.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define timeseries\n",
    "timeseries = tf.data.Dataset.range(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window:\n",
      "window 0 : [0 1 2 3 4 5 6 7 8 9]\n",
      "window 1 : [10 11 12 13 14 15 16 17 18 19]\n",
      "window 2 : [20 21 22 23 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "# Simple rolling window\n",
    "batches = timeseries.batch(10, drop_remainder=True)\n",
    "\n",
    "print('Rolling window:')\n",
    "for i, example in enumerate(batches.take(3)):\n",
    "    print('window', i, ':', example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split rolling window into input features and labels(predicted features):\n",
      "[0 1 2 3 4]  =>  [5 6 7 8 9]\n",
      "[10 11 12 13 14]  =>  [15 16 17 18 19]\n",
      "[20 21 22 23 24]  =>  [25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "# Rolling window, window = feature data + data to be predicted\n",
    "def label_next_5_steps(batch):\n",
    "    return (batch[:-5], batch[-5:])\n",
    "\n",
    "inputs_and_labels = batches.map(label_next_5_steps)\n",
    "\n",
    "print('Split rolling window into input features and labels(predicted features):')\n",
    "for input_feature, label in inputs_and_labels.take(3):\n",
    "    print(input_feature.numpy(), \" => \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window:\n",
      "window 0 : [ 0  3  6  9 12 15 18 21]\n",
      "window 1 : [ 2  5  8 11 14 17 20 23]\n",
      "window 2 : [ 4  7 10 13 16 19 22 25]\n",
      "\n",
      "Split rolling window into input features and labels(predicted features):\n",
      "[ 0  3  6  9 12] => [15 18 21]\n",
      "[ 2  5  8 11 14] => [17 20 23]\n",
      "[ 4  7 10 13 16] => [19 22 25]\n"
     ]
    }
   ],
   "source": [
    "# Rolling window generalized\n",
    "# Create dataset of windows out of timeseries dataset\n",
    "input_feature_steps = 5\n",
    "predict_steps = 3\n",
    "stride = 3\n",
    "shift = 2\n",
    "window_size = input_feature_steps + predict_steps\n",
    "windows = timeseries.window(size=window_size,\n",
    "                            shift=shift,\n",
    "                            stride=stride)\n",
    "\n",
    "# Batchify then flatten the dataset of batches into a dataset of their elements\n",
    "windows = windows.flat_map(lambda x: x.batch(window_size, drop_remainder=True))\n",
    "\n",
    "print('Rolling window:')\n",
    "for i, example in enumerate(windows.take(3)):\n",
    "    print('window', i, ':', example.numpy())\n",
    "\n",
    "# Split rolling window into input features and labels(predicted features)\n",
    "def split_at_n(batch, n):\n",
    "    return batch[:-n], batch[-n:]\n",
    "\n",
    "inputs_and_labels = windows.map(lambda x: split_at_n(x, n=predict_steps))\n",
    "\n",
    "print()\n",
    "print('Split rolling window into input features and labels(predicted features):')\n",
    "for input_feature, label in inputs_and_labels.take(3):\n",
    "    print(input_feature.numpy(), \"=>\", label.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning tensorflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "array_a = np.array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n",
    "array_b = np.array([-1, -2, -3, -4, -5])\n",
    "print(array_a)\n",
    "print(array_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensorflow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((array_a, array_b))\n",
    "for a,b in dataset:\n",
    "    print(a.numpy(), ' , ', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Apply Python functions\n",
    "def tf_some_python_function(input_a, input_b):\n",
    "    [input_a,] = tf.py_function(np.log1p, [input_a], [tf.float32])\n",
    "    return input_a, input_b\n",
    "\n",
    "mapped_dataset = dataset.map(tf_some_python_function)\n",
    "\n",
    "for a,b in mapped_dataset:\n",
    "    print(a.numpy(), ' , ', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches\n",
    "batch_size = 2\n",
    "take_n_batch = 5\n",
    "\n",
    "print('\\nBatched data, keep partial:')            \n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "\n",
    "for i, batch in enumerate(batched_dataset.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())\n",
    "        \n",
    "print('\\nBatched data, keep partial, repeat:')            \n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "batched_dataset = batched_dataset.repeat()\n",
    "\n",
    "for i, batch in enumerate(batched_dataset.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "# shuffle, batch, repeat order should be kept\n",
    "shuffled = dataset.shuffle(buffer_size=100)\n",
    "shuffled = shuffled.batch(batch_size)\n",
    "shuffled = shuffled.repeat()\n",
    "for i, batch in enumerate(shuffled.take(take_n_batch)):\n",
    "    print('batch', i)\n",
    "    for arr in batch:\n",
    "        print(arr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define timeseries\n",
    "timeseries = tf.data.Dataset.range(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple rolling window\n",
    "batches = timeseries.batch(10, drop_remainder=True)\n",
    "\n",
    "print('Rolling window:')\n",
    "for i, example in enumerate(batches.take(3)):\n",
    "    print('window', i, ':', example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window, window = feature data + data to be predicted\n",
    "def label_next_5_steps(batch):\n",
    "    return (batch[:-5], batch[-5:])\n",
    "\n",
    "inputs_and_labels = batches.map(label_next_5_steps)\n",
    "\n",
    "print('Split rolling window into input features and labels(predicted features):')\n",
    "for input_feature, label in inputs_and_labels.take(3):\n",
    "    print(input_feature.numpy(), \" => \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window generalized\n",
    "# Create dataset of windows out of timeseries dataset\n",
    "input_feature_steps = 5\n",
    "predict_steps = 3\n",
    "stride = 3\n",
    "shift = 2\n",
    "window_size = input_feature_steps + predict_steps\n",
    "windows = timeseries.window(size=window_size,\n",
    "                            shift=shift,\n",
    "                            stride=stride)\n",
    "\n",
    "# Batchify then flatten the dataset of batches into a dataset of their elements\n",
    "windows = windows.flat_map(lambda x: x.batch(window_size, drop_remainder=True))\n",
    "\n",
    "print('Rolling window:')\n",
    "for i, example in enumerate(windows.take(3)):\n",
    "    print('window', i, ':', example.numpy())\n",
    "\n",
    "# Split rolling window into input features and labels(predicted features)\n",
    "def split_at_n(batch, n):\n",
    "    return batch[:-n], batch[-n:]\n",
    "\n",
    "inputs_and_labels = windows.map(lambda x: split_at_n(x, n=predict_steps))\n",
    "\n",
    "print()\n",
    "print('Split rolling window into input features and labels(predicted features):')\n",
    "for input_feature, label in inputs_and_labels.take(3):\n",
    "    print(input_feature.numpy(), \"=>\", label.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator in 'real' action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data and labels\n",
    "def dummy_function(x):\n",
    "    a = 0.4 * np.sin(0.3 * x) * np.abs(x)\n",
    "    b = np.log1p(np.abs(x)) * np.cos(x)\n",
    "    c = np.exp(-x**2 / 100) * np.cos(x)\n",
    "    return [a, b, c]\n",
    "\n",
    "x = np.arange(-10, 10, 0.01)\n",
    "df = pd.DataFrame(np.array([dummy_function(xi) for xi in x]), columns=['Data_col_1', 'Data_col_2', 'Data_col_3'])\n",
    "\n",
    "df[['Data_col_1', 'Data_col_2', 'Data_col_3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window generalized\n",
    "# Create timeseries dataset\n",
    "timeseries = tf.data.Dataset.from_tensor_slices(df[['Data_col_1', 'Data_col_2', 'Data_col_3']].values)\n",
    "\n",
    "# Create dataset of windows out of timeseries dataset\n",
    "input_feature_steps = 1\n",
    "predict_steps = 3\n",
    "stride = 2\n",
    "shift = 3\n",
    "window_size = input_feature_steps + predict_steps\n",
    "windows = timeseries.window(size=window_size,\n",
    "                             shift=shift,\n",
    "                             stride=stride)\n",
    "\n",
    "# Batchify then flatten the dataset of batches into a dataset of their elements\n",
    "windows = windows.flat_map(lambda x: x.batch(window_size, drop_remainder=True))\n",
    "\n",
    "print('Rolling window:')\n",
    "for i, example in enumerate(windows.take(3)):\n",
    "    print('window', i, ':', example.numpy())\n",
    "\n",
    "# Split rolling window into input features and labels(predicted features)\n",
    "def split_at_n(batch, n):\n",
    "    return batch[:-n], batch[-n:]\n",
    "\n",
    "inputs_and_labels = windows.map(lambda x: split_at_n(x, n=predict_steps))\n",
    "\n",
    "print()\n",
    "print('Split rolling window into input features and labels(predicted features):')\n",
    "for input_feature, label in inputs_and_labels.take(3):\n",
    "    print(input_feature.numpy(), \"=>\", label.numpy())\n",
    "    \n",
    "# Make batches and shuffle before feeding the neural network\n",
    "inputs_and_labels = inputs_and_labels.batch(100)\n",
    "inputs_and_labels = inputs_and_labels.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "\n",
    "def create_model():   \n",
    "    input0 = Input(shape=(input_feature_steps, 3))\n",
    "    \n",
    "    flatten = Flatten() (input0)\n",
    "  \n",
    "    dense0 = Dense(128, activation='relu', name='1st') (flatten)\n",
    "  \n",
    "    dense1 = Dense(32, activation='relu', name='2nd') (dense0)\n",
    "    \n",
    "    output0 = Dense(predict_steps*3, name='3rd') (dense1)\n",
    "    output0 = Reshape((predict_steps, 3)) (output0)\n",
    "   \n",
    "    model = Model(input0, output0)\n",
    "  \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs_and_labels, epochs=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
